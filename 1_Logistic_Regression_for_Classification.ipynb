{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojr60iWHIoKJ"
      },
      "source": [
        "# Logistic Regression for Classification\n",
        "\n",
        "Classification is a fundamental task in supervised learning where we aim to predict categorical values. The predicted categories can be binary (e.g., true or false, 0 or 1, yes or no) or multi-class (e.g., identifying different species of flowers).\n",
        "\n",
        "Some examples of binary classification tasks include determining whether an email is spam or not, whether a transaction is fraudulent or legitimate, or whether a tumor is malignant or benign. In multi-class classification, we might aim to classify images of animals into different categories such as cats, dogs, or birds.\n",
        "\n",
        "Accurate classification models are essential in many applications such as healthcare, finance, and e-commerce, where making correct predictions can significantly impact outcomes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok4EAXA8Isul"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "To classify tumors as malignant or not, using a linear regression model may not be the best approach. Figure 1 shows a scatter plot with two types of data, \"O\" representing malignant tumors, and \"X\" representing non-malignant ones. When fitting a line to the data points and using it as a decision boundary with a threshold of 0.5, we can classify new data points as malignant or non-malignant. However, as shown in Figure 2, when we apply this model to new data, the decision boundary changes, and we may misclassify some data points, leading to serious errors like classifying a malignant tumor as non-malignant.\n",
        "\n",
        "Therefore, it is important to explore other models that can better handle the complexity of tumor classification, such as logistic regression. Logistic regression is specifically designed for binary classification tasks and can capture non-linear relationships between input features and tumor class, resulting in more accurate and reliable predictions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_rpNOGSanxWP"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "A logistic regression is a classification model that uses a sigmoid function to predict the probability of a binary outcome. Visually it is represented as an S-curve as shown in figure 3.\n",
        "\n",
        "This curve is called the logistic curve or the sigmoid curve. Mathematically, logistic regression model is represented as\n",
        "\n",
        "\\begin{align}\n",
        "  g(z)=\\frac{1}{1+e^{-z}}\n",
        "\\end{align}\n",
        "\n",
        "Where $z=\\vec{w} \\cdot \\vec{x}+b$ is the linear combination of the features $\\vec{x}$ with weights $\\vec{w}$ and bias $b$. The output of the sigmoid function $g(z)$ ranges from $0$ to $1$, with values closer to $1$ indicating a higher probability of the positive outcome.\n",
        "\n",
        "Notice in figure 3 that\n",
        "\n",
        "\\begin{equation}\n",
        "  & \\text{If } z=100, g(z)≈1\n",
        "  \\newline\n",
        "  \\\\ \\text{ and} \\\\\n",
        "  \\newline\n",
        "  & \\text{If } z=-100, g(z)≈0\n",
        "\\end{equation}\n",
        "\n",
        "When $z=0$ then $g(z)=0.5$ and that is why the sigmoid function always passes through 0.5 as shown in figure 3.\n",
        "\n",
        "We can build a logistic regression model using two steps.\n",
        "\n",
        "1. We build a linear regression model as\n",
        "\n",
        "\\begin{equation}\n",
        "z = \\vec{w}.\\vec{x}+b\n",
        "\\end{equation}\n",
        "\n",
        "2. Then we apply the sigmoid function as\n",
        "\n",
        "\\begin{align}\n",
        "  g(z)=\\frac{1}{1+e^{-1}}\n",
        "\\end{align}\n",
        "\n",
        "Putting them togather we have,\n",
        "\n",
        "\\begin{align}\n",
        "  f_{\\vec{w},b}(\\vec{x}) = g(z) = \\frac{1}{1+e^{-1}}\n",
        "\\end{align}\n",
        "\n",
        "Suppose we want to predict whether a tumor is malignant or not based on its size feature $\\vec{x}$. We can set a threshold of $0.5$ for the predicted probability, such that if $g(z) \\geq 0.5$, we predict a malignant tumor ($y=1$), and if $g(z) < 0.5$, we predict a non-malignant tumor ($y=0$).\n",
        "\n",
        "\\begin{equation}\n",
        "  & \\text{If } y>0.5 \\text{ then } y=1\n",
        "  \\newline\n",
        "  & \\text{If } y<0.5 \\text{ then } y=0\n",
        "\\end{equation}\n",
        "\n",
        "If $g(z)=0.7$, then we have a $70%$ probability of a malignant tumor, and a $30%$ probability of a non-malignant tumor. The probability of a non-malignant tumor can be calculated as\n",
        "\n",
        "\\begin{align}\n",
        "  P(y=0)=1-P(y=1)=1-0.7=0.3\n",
        "\\end{align}\n",
        "\n",
        "To predict $\\hat{y}$, we check whether $g(z) \\geq 0.5$, which is equivalent to $\\vec{w} \\cdot \\vec{x}+b \\geq 0$ when $y=1$, and $\\vec{w} \\cdot \\vec{x}+b < 0$ when $y=0$. This threshold determines the decision boundary between the two classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sqwin-wwnQL"
      },
      "source": [
        "### Decision Boundary\n",
        "\n",
        "A decision boundary is a boundry or hyperplane that saperates two or more different categories.\n",
        "\n",
        "Let's use two features $x_{1}$ and $x_{2}$ to predict of $y=0$ or $y=1$ as shown in figure 4. Suppose the data $Xs$ is to the right and $Os$ to the left. We want to fit the following model to the data points.\n",
        "\n",
        "\\begin{align}\n",
        "  f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_{1}x_{1}+w_{2}x_{2}+b)\n",
        "\\end{align}\n",
        "\n",
        "For now we assume the parameters $w_{1}, w_{2} = 1$ and $b=3$. We have, \n",
        "\n",
        "\\begin{align}\n",
        "x_{1}+x_{2}+3=0 \\\\\n",
        "\\\\\n",
        "x_{1}+x_{2}=-3\n",
        "\\end{align}\n",
        "\n",
        "We obtain the best fit line as shown in figure 5. This best fit line is the decision boundary which saperates the two types of data points. The data at the left of the decision boundary is predicted as $0$ and to the right is predicted as $1$. This decision boundary was for one set of parameter and will vary for some other set of parameters.\n",
        "\n",
        "The example we used here was for linear data but in real world problems we will often have non-linear data as shown in figure 6. For the non-linear problems, we will use some other non-linear model such as polynomial regression to make a decision boundary.\n",
        "\n",
        "The polynomial regression model is represented as\n",
        "\n",
        "\\begin{align}\n",
        "  f_{\\vec{w},b}(\\vec{x}) = g(z) = g(w_{1}x^{2}_{1}+w_{2}x^{2}_{2}+b)\n",
        "\\end{align}\n",
        "\n",
        "For this we also assume the parameters $w_{1}, w_{2} = 1$ except $b=-1$. We have,\n",
        "\n",
        "\\begin{align}\n",
        "x^{2}_{1}+x^{2}_{2}-1=0 \\\\\n",
        "\\\\\n",
        "x^{2}_{1}+x^{2}_{2}=1\n",
        "\\end{align}\n",
        "\n",
        "Since we have equation of circle, we obtain a circular decision boundary Inside the circular the data predicted is 1 and outside the circle it is 0.\n",
        "\n",
        "These were two types of decision boundary but we can have other types of complex decision boundary depending upon the complexity of the data and the model being fit to the data. Some of them are shown in figure 7.\n",
        "\n",
        "Determining the optimal decision boundary is a critical aspect of machine learning models, and optimizing the model's parameters is often necessary to achieve this. Techniques such as gradient descent are commonly used to optimize the parameters of the model and find the best decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnavWJS-iRLx"
      },
      "source": [
        "### Cost Function\n",
        "\n",
        "The Least Squares Method, which is commonly used for linear regression, is not suitable for the logistic regression model. This is because logistic regression involves a nonlinear relationship between the predictors and the outcome, and the least squares method assumes a linear relationship. Therefore, the least squares method will not provide the optimal parameters needed for logistic regression.\n",
        "\n",
        "To optimize the logistic regression model, we need to define a cost function that measures the error between the predicted and actual values. The cost function is defined as the average of the loss function for each data point. The loss function measures the difference between the predicted probability of a positive example and the true label.\n",
        "\n",
        "The logistic regression cost function is defined as follows:\n",
        "\n",
        "\\begin{align}\n",
        "J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}L(f_{\\vec{w},b}(\\vec{x}^{i}),y^{i})\n",
        "\\end{align}\n",
        "\n",
        "Where $L(f_{\\vec{w},b}(\\vec{x}^{i}),y^{i})$ is the loss function which is defined as\n",
        "\n",
        "\\begin{align}\n",
        "L(f_{\\vec{w},b}(\\vec{x}^{i}),y^{i}) = -y^{i}log(f_{\\vec{w},b}(\\vec{x}^{i})) - (1-y^{i})log(1 - f_{\\vec{w},b}(\\vec{x}^{i}))\n",
        "\\end{align}\n",
        "\n",
        "This loss function is derived from a statistical method called the maximum likelihood. It measures the logarithmic loss between the predicted probability and the true label. When the true label is positive ($y=1$), the loss function penalizes the predicted probability if it is close to 0, and when the true label is negative ($y=0$), the loss function penalizes the predicted probability if it is close to 1.\n",
        "\n",
        "By minimizing the cost function, we can find the optimal values of $\\vec{w}$ and $b$ that give the best predictions for our logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4TSJWf8L1c3"
      },
      "source": [
        "### Training Logistic Regression with Gradient Descent\n",
        "\n",
        "The goal of the gradient descent algorithm is to iteratively update the parameters $\\vec{w}$ and $b$ in order to minimize the cost function $J(\\vec{w},b)$. The algorithm achieves this by computing the gradient of the cost function with respect to the parameters and updating them in the opposite direction of the gradient. The optimal parameters $\\vec{w}$ and $b$ are reached when the cost function reaches its minimum value.\n",
        "\n",
        "The gradient descent algorithm is implemented as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{repeat } { \\\n",
        "\\qquad w_{j} = w_{j} - \\alpha\\frac{\\partial}{\\partial w_{j}}J(\\vec{w},b) \\\n",
        "\\qquad b = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b) \\\n",
        "}\n",
        "\\end{equation}\n",
        "\n",
        "Here, $\\frac{\\partial}{\\partial w_{j}}J$ and $\\frac{\\partial}{\\partial b}J$ are the partial derivatives of the cost function with respect to the parameters. They are defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial w_{j}}J=\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^i) - y^{i})x_{j}^{i} \\\n",
        "\\frac{\\partial}{\\partial b}J=\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^i) - y^{i})\n",
        "\\end{equation}\n",
        "\n",
        "The definition of $f_{\\vec{w},b}(\\vec{x}^{i})$ in logistic regression is different from that of linear regression.\n",
        "\n",
        "To make the gradient descent algorithm converge faster, we can use vectorized methods and feature scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg-iA73xM47e"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In summary, classification is a fundamental task in supervised learning used to predict categorical values, which can be binary or multi-class. Accurate classification models are essential in various fields such as healthcare, finance, and e-commerce. Logistic regression is a classification model that uses a sigmoid function to predict the probability of a binary outcome. It is specifically designed for binary classification tasks and can capture non-linear relationships between input features and tumor class, resulting in more accurate and reliable predictions. The cost function for logistic regression measures the error between the predicted and actual values, and it is defined as the average of the loss function for each data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2lRwUoVIdVt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
