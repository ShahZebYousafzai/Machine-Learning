{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy/qfhA2L85b3AllHICTGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahZebYousafzai/Machine-Learning/blob/main/2_Solving_Overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling the Problem of Overfitting\n",
        "\n",
        "When constructing and training a machine learning model, encountering challenges is inevitable. It's not uncommon for the model to initially underperform, but with further refinement and adjustment, it can eventually yield improved results.\n",
        "\n",
        "Let's examine the three possible scenarios we may encounter. For instance, suppose we aim to forecast house prices based on their size, using a linear regression model depicted in Figure 1.\n",
        "\n",
        "If the data is non-linear, trying to fit it with a linear regression model is not a wise decision since it will result in poor performance on the training data. This situation is referred to as underfitting or high bias.\n",
        "\n",
        "To improve the model, let's try fitting a quadratic function to the training data, as shown in Figure 2. Although it does not fit the training data perfectly, it outperforms the previous model, and it also generalizes well on the testing set.\n",
        "\n",
        "However, let's say we attempt to fit a 4th order polynomial to the data, as displayed in Figure 3. This leads to a wiggly curve that fits the training data extremely well, but it is not applicable to the test set. This issue is known as overfitting or high variance. In this situation our model will noit generalize well on the testing data.\n",
        "\n",
        "Therefore, the best scenario for our problem is the second one, where the model fits the data well without overfitting or underfitting.\n",
        "\n",
        "The same concept applies to classification, and we would like to resolve this issue as well.\n",
        "\n",
        "Underfitting, also known as high bias, occurs when our model does not perform well on either the training data or the testing data. On the other hand, overfitting, also known as high variance, arises when our model performs exceptionally well on the training data but performs poorly on the testing data.\n",
        "\n",
        "Encountering the underfitting scenario is less common, while the overfitting scenario is more likely to occur in practice which we will need to solve.\n",
        "\n",
        "To address the overfitting problem, we have several options available:\n",
        "\n",
        "1. Adding more training examples can help, but it is often time-consuming and costly since it requires extensive labor work.\n",
        "\n",
        "2. Adding training data in unstructured data is comparatively easier than in structured data. If we possess domain knowledge for our problem, we can add or remove some features. However, adding more features can lead us towards overfitting, while removing features may cause us to discard important features. Hence, this may not be the best option.\n",
        "\n",
        "3. The most common solution to overfitting is regularization, which we will discuss."
      ],
      "metadata": {
        "id": "5ce6lRUDVKhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "\n",
        "Regularization is a technique that aims to reduce the size of the model's parameter values, particularly the parameter $w_j$. Rather than eliminating features, it reduces the values of the parameters that have excessively large values. This approach helps prevent overfitting and enhances the model's generalization performance on new, unseen data.\n",
        "\n",
        "Suppose we fit a higher-order polynomial model, represented as \n",
        "\n",
        "\\begin{aligned}\n",
        "w_{1}x + w_{2}x^{2} + w_{3}x^{3} + w_{4}x^{4} + b\n",
        "\\end{aligned}\n",
        "\n",
        "to a training dataset, as shown in Figure 4. However, this model is overfitting the data. When $x^{3}$ and $x^{4}$ become very large, we would like to minimize the parameters $w_{3}$ and $w_{4}$ as much as possible or bring them close to 0 e.g; 0.001. As a result, the terms $w_{3}x^{3} + w_{4}x^{4}$ will be cancelled out, and we will obtain a quadratic equation, \n",
        "\n",
        "\\begin{aligned}\n",
        "w_{1}x + w_{2}x^{2} + b\n",
        "\\end{aligned}\n",
        "\n",
        "However, note that $w_{3}x^{3}$ and $w_{4}x^{4}$ will still have a slight contribution, but it will be significantly reduced.\n",
        "\n",
        "We can write the regulrized cost function for the higher order polynomial as\n",
        "\n",
        "\\begin{aligned}\n",
        "J(\\vec{w},b)=\\underset{\\vec{w},b}{min} \\frac{1}{2m} \\sum^{m}_{i=1}(f_{\\vec{w},b}(\\vec{x}^{2})-y^{i})^{2} + 1000\\underbrace{w^{2}_{3}}_{0.001} + 1000\\underbrace{w^{2}_{4}}_{0.001}\n",
        "\\end{aligned}\n",
        "\n",
        "However, in practice, we may have hundreds or thousands of features, making it difficult to determine which ones to include or exclude. To penalize all features, we can add a regularization term $\\frac{λ}{2m}\\sum_{j=1}^{n}w_{j}^{2}$ to the cost function.\n",
        "\n",
        "\\begin{aligned}\n",
        "J(\\vec{w},b)=\\underset{\\vec{w},b}{min} \\frac{1}{2m} \\sum^{m}_{i=1}(f_{\\vec{w},b}(\\vec{x}^{2})-y^{i})^{2} + \\frac{λ}{2m}\\sum_{j=1}^{n}w_{j}^{2}\n",
        "\\end{aligned}\n",
        "\n",
        "Where $λ$ is the regularization parameter. \n",
        "\n",
        "So now there are two goals that need to be minimized\n",
        "\n",
        "* Mean Squared Error\n",
        "\n",
        "\\begin{aligned}\n",
        "J(\\vec{w},b)=\\underset{\\vec{w},b}{min} \\frac{1}{2m} \\sum^{m}_{i=1}(f_{\\vec{w},b}(\\vec{x}^{2})-y^{i})^{2}\n",
        "\\end{aligned}\n",
        "\n",
        "* Regularization Term\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{λ}{2m}\\sum_{j=1}^{n}w_{j}^{2}\n",
        "\\end{aligned}\n",
        "\n",
        "The $λ$ controls the strength of the regularization that needs to be applied. If $\\lambda=0$, no regularization is applied and we get the cost function without the regularization term. If $\\lambda$ is set very high, the regularization term will dominate the cost function and the model will underfit.\n",
        "\n",
        "So we need to fit the model and at the same time keep the parameters $w_{j}$ small. $λ$ helps in balancing both of them but we will need a value for it which will balance both of them.\n"
      ],
      "metadata": {
        "id": "_VKF3jDFetNA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbGqzUkteslB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}